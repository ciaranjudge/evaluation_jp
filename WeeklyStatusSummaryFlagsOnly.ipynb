{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weekly Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd              # python package for dataframes\n",
    "import os                        # used to change directory paths\n",
    "import matplotlib.pyplot as plt  # python package for plotting\n",
    "import numpy as np\n",
    "import seaborn as sns #package for plotting\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "from IPython.display import display, HTML  # Make tables pretty\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zipped csv, output from SAS project, Jpathquarters [\\\\cskma0294\\F\\SH\\Jpathquarters.egp] imported below\n",
    "\n",
    "<em>Commented out temporarily</em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#zf = zipfile.ZipFile('\\\\\\\\cskma0294\\\\F\\\\Evaluations\\\\JobPath\\\\jp_outcomes.zip')\n",
    "#df = pd.read_csv(zf.open('jp_outcomes.csv'))\n",
    "\n",
    "# filename_jp_zip = 'jp_outcomes.csv'\n",
    "# path_jp_zip = '\\\\\\\\cskma0294\\\\F\\\\Evaluations\\\\JobPath'\n",
    "\n",
    "# data_jp = pd.read_csv(path_jp_zip+filename_jp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import WeeklyStatus.csv into a DataFrame and tidy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import WeeklyStatus.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zf2 = zipfile.ZipFile('\\\\\\\\cskma0294\\\\F\\\\Evaluations\\\\JobPath\\\\\\Quarterly_status\\\\WeeklyStatus.zip')\n",
    "df2 = pd.read_csv(zf2.open('WeeklyStatus.csv'))\n",
    "\n",
    "df2.head()         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename 'StatusEnd_ddmmmyyyy' column names\n",
    "Change 'StatusEnd_ddmmmyyyy' column name strings to integers of the form yyyymmdd, which can then be ordered by date and – if converted to integers with <em>int()</em> – manipulated numerically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_columns = df2.columns.values\n",
    "new_columns = original_columns.copy()\n",
    "\n",
    "months = {\n",
    "    \"Jan\": \"01\",\n",
    "    \"Feb\": \"02\",\n",
    "    \"Mar\": \"03\",\n",
    "    \"Apr\": \"04\",\n",
    "    \"May\": \"05\",\n",
    "    \"Jun\": \"06\",\n",
    "    \"Jul\": \"07\",\n",
    "    \"Aug\": \"08\",\n",
    "    \"Sep\": \"09\",\n",
    "    \"Oct\": \"10\",\n",
    "    \"Nov\": \"11\",\n",
    "    \"Dec\": \"12\"\n",
    "}\n",
    "\n",
    "for i in range(2, len(original_columns)):\n",
    "    year = original_columns[i][15:]\n",
    "    month = months[original_columns[i][12:15]]\n",
    "    day = original_columns[i][10:12]\n",
    "    new_columns[i] = year + month + day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2.columns = new_columns\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort columns by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_columns = np.append(new_columns[:2], sorted(new_columns[2:]))\n",
    "df2 = df2[sorted_columns]\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the DataFrame so that it has a single date column and a single status column\n",
    "### Split the DataFrame into 10 parts\n",
    "Split the DataFrame in 10 parts across its width and melt. This is to ease memory requirements while processing.\n",
    "\n",
    "First we separate the ppsn column into a DataFrame <em>df_id</em>, which we can concatenate to each of the segments of the original DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_id = df2.iloc[:,0:1]\n",
    "df2_id.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the DataFrame into parts, concatenate <em>df_id</em> to the left of each part and melt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoffs = []\n",
    "for i in range (1,10): \n",
    "    cutoffs.append(int(i*240/10))\n",
    "    \n",
    "cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "\n",
    "df3_part1 = pd.melt(pd.concat([df2_id, df2.iloc[:,2:cutoffs[j]]], axis=1), \n",
    "                    id_vars = ['ppsn'], var_name = 'date', value_name = 'status_codes')\n",
    "df3_part2 = pd.melt(pd.concat([df2_id, df2.iloc[:,cutoffs[j]:cutoffs[j+1]]], axis=1), \n",
    "                    id_vars = ['ppsn'], var_name = 'date', value_name = 'status_codes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_part1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_part2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Melt each of the parts into long DataFrames with a single date column and a single status column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = j+1\n",
    "df3_part3 = pd.melt(pd.concat([df2_id, df2.iloc[:,cutoffs[j]:cutoffs[j+1]]], axis=1), \n",
    "                    id_vars = ['ppsn'], var_name = 'date', value_name = 'status_codes')\n",
    "j = j+1\n",
    "df3_part4 = pd.melt(pd.concat([df2_id, df2.iloc[:,cutoffs[j]:cutoffs[j+1]]], axis=1), \n",
    "                    id_vars = ['ppsn'], var_name = 'date', value_name = 'status_codes')\n",
    "j = j+1\n",
    "df3_part5 = pd.melt(pd.concat([df2_id, df2.iloc[:,cutoffs[j]:cutoffs[j+1]]], axis=1), \n",
    "                    id_vars = ['ppsn'], var_name = 'date', value_name = 'status_codes')\n",
    "j = j+1\n",
    "df3_part6 = pd.melt(pd.concat([df2_id, df2.iloc[:,cutoffs[j]:cutoffs[j+1]]], axis=1), \n",
    "                    id_vars = ['ppsn'], var_name = 'date', value_name = 'status_codes')\n",
    "j = j+1\n",
    "df3_part7 = pd.melt(pd.concat([df2_id, df2.iloc[:,cutoffs[j]:cutoffs[j+1]]], axis=1), \n",
    "                    id_vars = ['ppsn'], var_name = 'date', value_name = 'status_codes')\n",
    "j = j+1\n",
    "df3_part8 = pd.melt(pd.concat([df2_id, df2.iloc[:,cutoffs[j]:cutoffs[j+1]]], axis=1), \n",
    "                    id_vars = ['ppsn'], var_name = 'date', value_name = 'status_codes')\n",
    "j = j+1\n",
    "df3_part9 = pd.melt(pd.concat([df2_id, df2.iloc[:,cutoffs[j]:cutoffs[j+1]]], axis=1), \n",
    "                    id_vars = ['ppsn'], var_name = 'date', value_name = 'status_codes')\n",
    "\n",
    "df3_part10 = pd.melt(pd.concat([df2_id, df2.iloc[:,cutoffs[j+1]:]], axis=1), \n",
    "                     id_vars = ['ppsn'], var_name = 'date', value_name = 'status_codes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export as gzipped CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = str(datetime.datetime.now()).split('.')[0][0:10]\n",
    "\n",
    "df3_part1.to_csv('\\\\\\\\cskma0294\\\\F\\\\Evaluations\\\\JobPath\\\\WeeklyStatus_part1_melt_'+today+'.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_part2.to_csv('\\\\\\\\cskma0294\\\\F\\\\Evaluations\\\\JobPath\\\\WeeklyStatus_part2_melt_'+today+'.gz', compression='gzip')\n",
    "df3_part3.to_csv('\\\\\\\\cskma0294\\\\F\\\\Evaluations\\\\JobPath\\\\WeeklyStatus_part3_melt_'+today+'.gz', compression='gzip')\n",
    "df3_part4.to_csv('\\\\\\\\cskma0294\\\\F\\\\Evaluations\\\\JobPath\\\\WeeklyStatus_part4_melt_'+today+'.gz', compression='gzip')\n",
    "df3_part5.to_csv('\\\\\\\\cskma0294\\\\F\\\\Evaluations\\\\JobPath\\\\WeeklyStatus_part5_melt_'+today+'.gz', compression='gzip')\n",
    "df3_part6.to_csv('\\\\\\\\cskma0294\\\\F\\\\Evaluations\\\\JobPath\\\\WeeklyStatus_part6_melt_'+today+'.gz', compression='gzip')\n",
    "df3_part7.to_csv('\\\\\\\\cskma0294\\\\F\\\\Evaluations\\\\JobPath\\\\WeeklyStatus_part7_melt_'+today+'.gz', compression='gzip')\n",
    "df3_part8.to_csv('\\\\\\\\cskma0294\\\\F\\\\Evaluations\\\\JobPath\\\\WeeklyStatus_part8_melt_'+today+'.gz', compression='gzip')\n",
    "df3_part9.to_csv('\\\\\\\\cskma0294\\\\F\\\\Evaluations\\\\JobPath\\\\WeeklyStatus_part9_melt_'+today+'.gz', compression='gzip')\n",
    "df3_part10.to_csv('\\\\\\\\cskma0294\\\\F\\\\Evaluations\\\\JobPath\\\\WeeklyStatus_part10_melt_'+today+'.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>Restart Kernel, clear output, check date of zipped files and proceed</font>\n",
    "\n",
    "### Reset environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd              # python package for dataframes\n",
    "import os                        # used to change directory paths\n",
    "#import matplotlib.pyplot as plt  # python package for plotting\n",
    "#import numpy as np\n",
    "#import seaborn as sns #package for plotting\n",
    "#from scipy.stats import norm\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#from scipy import stats\n",
    "#from IPython.display import display, HTML  # Make tables pretty\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "#import zipfile\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Import 1st gzipped CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = str(datetime.now()).split('.')[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('\\\\\\\\cskma0294\\\\F\\\\Evaluations\\\\JobPath\\\\WeeklyStatus_part1_melt_'+today+'.gz') as f:\n",
    "    df3_part1 = pd.read_csv(f).iloc[:,1:] # drop 1st column (an artefact of the zipping process)\n",
    "                            \n",
    "df3_part1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split status codes\n",
    "Code to split a string containing multiple status codes into a list of single status codes of length n.\n",
    "\n",
    "<em>str_split_in_n</em> splits a string using <em>sep</em> into a list of <em>n</em> substrings.\n",
    "\n",
    "Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create summary flags for different status codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial thoughts on flags:<br> \n",
    "EMPL==1 if values in FIS C-UA C-UB WFP BTW<br>\n",
    "    WSWP-LF==1 if values in JA JB JST C-UA C-UB BTW<br>\n",
    "    WSWP=1 if values in DA IP OFP CA Mat<br>\n",
    "    EducTrain==1 if values in BTEA MOM<br>\n",
    "    LR flag==1 if values in C-UA C-UB UBCO UA UB<br>\n",
    "    CasualFlag==1 if values in C-UA C-UB<br>\n",
    "    CreditsFlag==1 if values in UBCO<br>\n",
    "    OverlapFlag==1 if WSWP-LF==1 and WSWP==1<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of 5 flags \n",
    "(differs a little from the above note)\n",
    "\n",
    "<em>substring_in_list</em> returns 1 if string <em>x</em>, when separated into parts using <em>sep</em>, contains a part that matches an element in the list <em>l</em>. Otherwise it returns 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substring_in_list (x, l, sep='+'):\n",
    "    s = str.split(str(x),sep)\n",
    "    match = 0\n",
    "    for i in s:\n",
    "        if i in l:\n",
    "            match = 1\n",
    "    return match\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substring_in_list('C-UA+C-A',['C-A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LM_WSW_list = ['C-UA', 'C-UB', 'FASS', 'FISH', 'UA', 'UB', 'INTN', 'SEMP', 'SST', 'YDI', 'STEA', 'SMLH']\n",
    "WSW_Non_LM_list = ['CA', 'DA', 'IP', 'OFP', 'OFPJST', 'PRTA', 'SPC', 'WCP', 'O 65', 'SPNC', 'SPT', 'WPGO']\n",
    "Ed_or_Training_list = ['BTE', 'SPFT']\n",
    "Empl_list = ['BTW', 'C-UA', 'C-UB', 'FIS', 'INTN', 'PTJI', 'SEMP', 'SST', 'STEA']\n",
    "LR_list = ['C-UA', 'C-UB', 'UA', 'UB', 'UBCO']\n",
    "\n",
    "df3_part1['LM_WSW'] = df3_part1.apply(lambda row: substring_in_list(row['status_codes'],LM_WSW_list, sep='+'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_part1['WSW_Non_LM'] = df3_part1.apply(lambda row: substring_in_list(row['status_codes'],WSW_Non_LM_list, sep='+'), axis=1)\n",
    "\n",
    "df3_part1['Ed_or_Training'] = df3_part1.apply(lambda row: substring_in_list(row['status_codes'],Ed_or_Training_list, sep='+'), \n",
    "                                              axis=1)\n",
    "\n",
    "df3_part1['Empl'] = df3_part1.apply(lambda row: substring_in_list(row['status_codes'],Empl_list, sep='+'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_part1['LR'] = df3_part1.apply(lambda row: substring_in_list(row['status_codes'],LR_list, sep='+'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_part1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_part1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_part1['LM_WSW'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_part1['WSW_Non_LM'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_part1['Ed_or_Training'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_part1['Empl'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export DataFrame to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "df3_part1.to_csv('\\\\\\\\cskma0294\\\\F\\\\Evaluations\\\\JobPath\\\\WeeklyStatus_flags_part1_'+today+'.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat for parts 2-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('\\\\\\\\cskma0294\\\\F\\\\Evaluations\\\\JobPath\\\\WeeklyStatus_part2_melt_'+today+'.gz') as f:\n",
    "    df3_part2 = pd.read_csv(f).iloc[:,1:] # drop 1st column (an artefact of the zipping process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_part2['LM_WSW'] = df3_part2.apply(lambda row: substring_in_list(row['status_codes'],LM_WSW_list, sep='+'), axis=1)\n",
    "\n",
    "df3_part2['WSW_Non_LM'] = df3_part2.apply(lambda row: substring_in_list(row['status_codes'],WSW_Non_LM_list, sep='+'), axis=1)\n",
    "\n",
    "df3_part2['Ed_or_Training'] = df3_part2.apply(lambda row: substring_in_list(row['status_codes'],Ed_or_Training_list, sep='+'), \n",
    "                                              axis=1)\n",
    "\n",
    "df3_part2['Empl'] = df3_part2.apply(lambda row: substring_in_list(row['status_codes'],Empl_list, sep='+'), axis=1)\n",
    "\n",
    "df3_part2['LR'] = df3_part2.apply(lambda row: substring_in_list(row['status_codes'],LR_list, sep='+'), axis=1)\n",
    "\n",
    "df3_part2.to_csv('\\\\\\\\cskma0294\\\\F\\\\Evaluations\\\\JobPath\\\\WeeklyStatus_flags_part2_'+today+'.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('\\\\\\\\cskma0294\\\\F\\\\Evaluations\\\\JobPath\\\\WeeklyStatus_part3_melt_'+today+'.gz') as f:\n",
    "    df3_part3 = pd.read_csv(f).iloc[:,1:] # drop 1st column (an artefact of the zipping process)\n",
    "\n",
    "with gzip.open('\\\\\\\\cskma0294\\\\F\\\\Evaluations\\\\JobPath\\\\WeeklyStatus_part4_melt_'+today+'.gz') as f:\n",
    "    df3_part4 = pd.read_csv(f).iloc[:,1:] # drop 1st column (an artefact of the zipping process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_part3['LM_WSW'] = df3_part3.apply(lambda row: substring_in_list(row['status_codes'],LM_WSW_list, sep='+'), axis=1)\n",
    "\n",
    "df3_part3['WSW_Non_LM'] = df3_part3.apply(lambda row: substring_in_list(row['status_codes'],WSW_Non_LM_list, sep='+'), axis=1)\n",
    "\n",
    "df3_part3['Ed_or_Training'] = df3_part3.apply(lambda row: substring_in_list(row['status_codes'],Ed_or_Training_list, sep='+'), \n",
    "                                              axis=1)\n",
    "\n",
    "df3_part3['Empl'] = df3_part3.apply(lambda row: substring_in_list(row['status_codes'],Empl_list, sep='+'), axis=1)\n",
    "\n",
    "df3_part3['LR'] = df3_part3.apply(lambda row: substring_in_list(row['status_codes'],LR_list, sep='+'), axis=1)\n",
    "\n",
    "df3_part3.to_csv('\\\\\\\\cskma0294\\\\F\\\\Evaluations\\\\JobPath\\\\WeeklyStatus_flags_part3_'+today+'.gz', compression='gzip')\n",
    "df3_part4['LM_WSW'] = df3_part4.apply(lambda row: substring_in_list(row['status_codes'],LM_WSW_list, sep='+'), axis=1)\n",
    "\n",
    "df3_part4['WSW_Non_LM'] = df3_part4.apply(lambda row: substring_in_list(row['status_codes'],WSW_Non_LM_list, sep='+'), axis=1)\n",
    "\n",
    "df3_part4['Ed_or_Training'] = df3_part4.apply(lambda row: substring_in_list(row['status_codes'],Ed_or_Training_list, sep='+'), \n",
    "                                              axis=1)\n",
    "\n",
    "df3_part4['Empl'] = df3_part4.apply(lambda row: substring_in_list(row['status_codes'],Empl_list, sep='+'), axis=1)\n",
    "\n",
    "df3_part4['LR'] = df3_part4.apply(lambda row: substring_in_list(row['status_codes'],LR_list, sep='+'), axis=1)\n",
    "\n",
    "df3_part4.to_csv('\\\\\\\\cskma0294\\\\F\\\\Evaluations\\\\JobPath\\\\WeeklyStatus_flags_part4_'+today+'.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat for parts 5-7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>Restart Kernel, clear output, check date of zipped files and proceed</font>\n",
    "\n",
    "### Reset environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd              # python package for dataframes\n",
    "import os                        # used to change directory paths\n",
    "#import matplotlib.pyplot as plt  # python package for plotting\n",
    "#import numpy as np\n",
    "#import seaborn as sns #package for plotting\n",
    "#from scipy.stats import norm\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#from scipy import stats\n",
    "#from IPython.display import display, HTML  # Make tables pretty\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "#import zipfile\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substring_in_list (x, l, sep='+'):\n",
    "    s = str.split(str(x),sep)\n",
    "    match = 0\n",
    "    for i in s:\n",
    "        if i in l:\n",
    "            match = 1\n",
    "    return match\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LM_WSW_list = ['C-UA', 'C-UB', 'FASS', 'FISH', 'UA', 'UB', 'INTN', 'SEMP', 'SST', 'YDI', 'STEA', 'SMLH']\n",
    "WSW_Non_LM_list = ['CA', 'DA', 'IP', 'OFP', 'OFPJST', 'PRTA', 'SPC', 'WCP', 'O 65', 'SPNC', 'SPT', 'WPGO']\n",
    "Ed_or_Training_list = ['BTE', 'SPFT']\n",
    "Empl_list = ['BTW', 'C-UA', 'C-UB', 'FIS', 'INTN', 'PTJI', 'SEMP', 'SST', 'STEA']\n",
    "LR_list = ['C-UA', 'C-UB', 'UA', 'UB', 'UBCO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = str(datetime.now()).split('.')[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('\\\\\\\\cskma0294\\\\F\\\\Evaluations\\\\JobPath\\\\WeeklyStatus_part5_melt_'+today+'.gz') as f:\n",
    "    df3_part5 = pd.read_csv(f).iloc[:,1:] # drop 1st column (an artefact of the zipping process)\n",
    "                            \n",
    "with gzip.open('\\\\\\\\cskma0294\\\\F\\\\Evaluations\\\\JobPath\\\\WeeklyStatus_part6_melt_'+today+'.gz') as f:\n",
    "    df3_part6 = pd.read_csv(f).iloc[:,1:] # drop 1st column (an artefact of the zipping process)\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_part5['LM_WSW'] = df3_part5.apply(lambda row: substring_in_list(row['status_codes'],LM_WSW_list, sep='+'), axis=1)\n",
    "\n",
    "df3_part5['WSW_Non_LM'] = df3_part5.apply(lambda row: substring_in_list(row['status_codes'],WSW_Non_LM_list, sep='+'), axis=1)\n",
    "\n",
    "df3_part5['Ed_or_Training'] = df3_part5.apply(lambda row: substring_in_list(row['status_codes'],Ed_or_Training_list, sep='+'), \n",
    "                                              axis=1)\n",
    "\n",
    "df3_part5['Empl'] = df3_part5.apply(lambda row: substring_in_list(row['status_codes'],Empl_list, sep='+'), axis=1)\n",
    "\n",
    "df3_part5['LR'] = df3_part5.apply(lambda row: substring_in_list(row['status_codes'],LR_list, sep='+'), axis=1)\n",
    "\n",
    "df3_part5.to_csv('\\\\\\\\cskma0294\\\\F\\\\Evaluations\\\\JobPath\\\\WeeklyStatus_flags_part5_'+today+'.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_part6['LM_WSW'] = df3_part6.apply(lambda row: substring_in_list(row['status_codes'],LM_WSW_list, sep='+'), axis=1)\n",
    "\n",
    "df3_part6['WSW_Non_LM'] = df3_part6.apply(lambda row: substring_in_list(row['status_codes'],WSW_Non_LM_list, sep='+'), axis=1)\n",
    "\n",
    "df3_part6['Ed_or_Training'] = df3_part6.apply(lambda row: substring_in_list(row['status_codes'],Ed_or_Training_list, sep='+'), \n",
    "                                              axis=1)\n",
    "\n",
    "df3_part6['Empl'] = df3_part6.apply(lambda row: substring_in_list(row['status_codes'],Empl_list, sep='+'), axis=1)\n",
    "\n",
    "df3_part6['LR'] = df3_part6.apply(lambda row: substring_in_list(row['status_codes'],LR_list, sep='+'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_part6.to_csv('\\\\\\\\cskma0294\\\\F\\\\Evaluations\\\\JobPath\\\\WeeklyStatus_flags_part6_'+today+'.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('\\\\\\\\cskma0294\\\\F\\\\Evaluations\\\\JobPath\\\\WeeklyStatus_part7_melt_'+today+'.gz') as f:\n",
    "    df3_part7 = pd.read_csv(f).iloc[:,1:] # drop 1st column (an artefact of the zipping process)\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_part7['LM_WSW'] = df3_part7.apply(lambda row: substring_in_list(row['status_codes'],LM_WSW_list, sep='+'), axis=1)\n",
    "\n",
    "df3_part7['WSW_Non_LM'] = df3_part7.apply(lambda row: substring_in_list(row['status_codes'],WSW_Non_LM_list, sep='+'), axis=1)\n",
    "\n",
    "df3_part7['Ed_or_Training'] = df3_part7.apply(lambda row: substring_in_list(row['status_codes'],Ed_or_Training_list, sep='+'), \n",
    "                                              axis=1)\n",
    "\n",
    "df3_part7['Empl'] = df3_part7.apply(lambda row: substring_in_list(row['status_codes'],Empl_list, sep='+'), axis=1)\n",
    "\n",
    "df3_part7['LR'] = df3_part7.apply(lambda row: substring_in_list(row['status_codes'],LR_list, sep='+'), axis=1)\n",
    "\n",
    "df3_part7.to_csv('\\\\\\\\cskma0294\\\\F\\\\Evaluations\\\\JobPath\\\\WeeklyStatus_flags_part7_'+today+'.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat for parts 8-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>Restart Kernel, clear output, check date of zipped files and proceed</font>\n",
    "\n",
    "### Reset environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd              # python package for dataframes\n",
    "import os                        # used to change directory paths\n",
    "#import matplotlib.pyplot as plt  # python package for plotting\n",
    "#import numpy as np\n",
    "#import seaborn as sns #package for plotting\n",
    "#from scipy.stats import norm\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#from scipy import stats\n",
    "#from IPython.display import display, HTML  # Make tables pretty\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "#import zipfile\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substring_in_list (x, l, sep='+'):\n",
    "    s = str.split(str(x),sep)\n",
    "    match = 0\n",
    "    for i in s:\n",
    "        if i in l:\n",
    "            match = 1\n",
    "    return match\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LM_WSW_list = ['C-UA', 'C-UB', 'FASS', 'FISH', 'UA', 'UB', 'INTN', 'SEMP', 'SST', 'YDI', 'STEA', 'SMLH']\n",
    "WSW_Non_LM_list = ['CA', 'DA', 'IP', 'OFP', 'OFPJST', 'PRTA', 'SPC', 'WCP', 'O 65', 'SPNC', 'SPT', 'WPGO']\n",
    "Ed_or_Training_list = ['BTE', 'SPFT']\n",
    "Empl_list = ['BTW', 'C-UA', 'C-UB', 'FIS', 'INTN', 'PTJI', 'SEMP', 'SST', 'STEA']\n",
    "LR_list = ['C-UA', 'C-UB', 'UA', 'UB', 'UBCO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = str(datetime.now()).split('.')[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('\\\\\\\\cskma0294\\\\F\\\\Evaluations\\\\JobPath\\\\WeeklyStatus_part8_melt_'+today+'.gz') as f:\n",
    "    df3_part8 = pd.read_csv(f).iloc[:,1:] # drop 1st column (an artefact of the zipping process)\n",
    "                            \n",
    "with gzip.open('\\\\\\\\cskma0294\\\\F\\\\Evaluations\\\\JobPath\\\\WeeklyStatus_part9_melt_'+today+'.gz') as f:\n",
    "    df3_part9 = pd.read_csv(f).iloc[:,1:] # drop 1st column (an artefact of the zipping process)\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_part8['LM_WSW'] = df3_part8.apply(lambda row: substring_in_list(row['status_codes'],LM_WSW_list, sep='+'), axis=1)\n",
    "\n",
    "df3_part8['WSW_Non_LM'] = df3_part8.apply(lambda row: substring_in_list(row['status_codes'],WSW_Non_LM_list, sep='+'), axis=1)\n",
    "\n",
    "df3_part8['Ed_or_Training'] = df3_part8.apply(lambda row: substring_in_list(row['status_codes'],Ed_or_Training_list, sep='+'), \n",
    "                                              axis=1)\n",
    "\n",
    "df3_part8['Empl'] = df3_part8.apply(lambda row: substring_in_list(row['status_codes'],Empl_list, sep='+'), axis=1)\n",
    "\n",
    "df3_part8['LR'] = df3_part8.apply(lambda row: substring_in_list(row['status_codes'],LR_list, sep='+'), axis=1)\n",
    "\n",
    "df3_part8.to_csv('\\\\\\\\cskma0294\\\\F\\\\Evaluations\\\\JobPath\\\\WeeklyStatus_flags_part8_'+today+'.gz', compression='gzip')\n",
    "\n",
    "df3_part9['LM_WSW'] = df3_part9.apply(lambda row: substring_in_list(row['status_codes'],LM_WSW_list, sep='+'), axis=1)\n",
    "\n",
    "df3_part9['WSW_Non_LM'] = df3_part9.apply(lambda row: substring_in_list(row['status_codes'],WSW_Non_LM_list, sep='+'), axis=1)\n",
    "\n",
    "df3_part9['Ed_or_Training'] = df3_part9.apply(lambda row: substring_in_list(row['status_codes'],Ed_or_Training_list, sep='+'), \n",
    "                                              axis=1)\n",
    "\n",
    "df3_part9['Empl'] = df3_part9.apply(lambda row: substring_in_list(row['status_codes'],Empl_list, sep='+'), axis=1)\n",
    "\n",
    "df3_part9['LR'] = df3_part9.apply(lambda row: substring_in_list(row['status_codes'],LR_list, sep='+'), axis=1)\n",
    "\n",
    "df3_part9.to_csv('\\\\\\\\cskma0294\\\\F\\\\Evaluations\\\\JobPath\\\\WeeklyStatus_flags_part9_'+today+'.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('\\\\\\\\cskma0294\\\\F\\\\Evaluations\\\\JobPath\\\\WeeklyStatus_part10_melt_'+today+'.gz') as f:\n",
    "    df3_part10 = pd.read_csv(f).iloc[:,1:] # drop 1st column (an artefact of the zipping process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_part10['LM_WSW'] = df3_part10.apply(lambda row: substring_in_list(row['status_codes'],LM_WSW_list, sep='+'), axis=1)\n",
    "\n",
    "df3_part10['WSW_Non_LM'] = df3_part10.apply(lambda row: substring_in_list(row['status_codes'],WSW_Non_LM_list, sep='+'), axis=1)\n",
    "\n",
    "df3_part10['Ed_or_Training'] = df3_part10.apply(lambda row: substring_in_list(row['status_codes'],Ed_or_Training_list, sep='+'), \n",
    "                                              axis=1)\n",
    "\n",
    "df3_part10['Empl'] = df3_part10.apply(lambda row: substring_in_list(row['status_codes'],Empl_list, sep='+'), axis=1)\n",
    "\n",
    "df3_part10['LR'] = df3_part10.apply(lambda row: substring_in_list(row['status_codes'],LR_list, sep='+'), axis=1)\n",
    "\n",
    "df3_part10.to_csv('\\\\\\\\cskma0294\\\\F\\\\Evaluations\\\\JobPath\\\\WeeklyStatus_flags_part10_'+today+'.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maybe: <font color=red>Restart Kernel, clear output, check date of zipped files and proceed</font>\n",
    "\n",
    "### If so, reset environment\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data frames needed for the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('\\\\\\\\cskma0294\\\\F\\\\Evaluations\\\\JobPath\\\\WeeklyStatus_flags_part1_'+today+'.gz') as f:\n",
    "    df3_part1 = pd.read_csv(f).iloc[:,1:] # drop 1st column (an artefact of the zipping process)\n",
    "with gzip.open('\\\\\\\\cskma0294\\\\F\\\\Evaluations\\\\JobPath\\\\WeeklyStatus_flags_part2_'+today+'.gz') as f:\n",
    "    df3_flags_part2 = pd.read_csv(f).iloc[:,1:] # drop 1st column (an artefact of the zipping process)\n",
    "with gzip.open('\\\\\\\\cskma0294\\\\F\\\\Evaluations\\\\JobPath\\\\WeeklyStatus_flags_part3_'+today+'.gz') as f:\n",
    "    df3_flags_part3 = pd.read_csv(f).iloc[:,1:] # drop 1st column (an artefact of the zipping process)\n",
    "with gzip.open('\\\\\\\\cskma0294\\\\F\\\\Evaluations\\\\JobPath\\\\WeeklyStatus_flags_part4_'+today+'.gz') as f:\n",
    "    df3_flags_part4 = pd.read_csv(f).iloc[:,1:] # drop 1st column (an artefact of the zipping process)\n",
    "with gzip.open('\\\\\\\\cskma0294\\\\F\\\\Evaluations\\\\JobPath\\\\WeeklyStatus_flags_part5_'+today+'.gz') as f:\n",
    "    df3_flags_part5 = pd.read_csv(f).iloc[:,1:] # drop 1st column (an artefact of the zipping process)\n",
    "with gzip.open('\\\\\\\\cskma0294\\\\F\\\\Evaluations\\\\JobPath\\\\WeeklyStatus_flags_part6_'+today+'.gz') as f:\n",
    "    df3_flags_part6 = pd.read_csv(f).iloc[:,1:] # drop 1st column (an artefact of the zipping process)\n",
    "with gzip.open('\\\\\\\\cskma0294\\\\F\\\\Evaluations\\\\JobPath\\\\WeeklyStatus_flags_part7_'+today+'.gz') as f:\n",
    "    df3_flags_part7 = pd.read_csv(f).iloc[:,1:] # drop 1st column (an artefact of the zipping process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine parts 1-10 and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.concat([df3_part1, df3_part2, df3_part3, df3_part4, df3_part5, \n",
    "                    df3_part6, df3_part7, df3_part8, df3_part9, df3_part10])\n",
    "\n",
    "df3.to_csv('\\\\\\\\cskma0294\\\\F\\\\Evaluations\\\\JobPath\\\\WeeklyStatus_flags'+today+'.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional additional status columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create columns for distinct status codes for an individual and timepoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to split a string containing multiple status codes into a list of single status codes of length n.\n",
    "\n",
    "<em>str_split_in_n</em> splits a string using <em>sep</em> into a list of <em>n</em> substrings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# str_split_in_n takes a string x and splits it into a sorted list using a separator (default value '+') of length n. \n",
    "# If the length of the list is less than n, them one or more nan values are addded.\n",
    "def str_split_in_n(x, n, sep='+', descend = False):\n",
    "    s = str.split(str(x),sep)\n",
    "    s.sort(reverse = descend)\n",
    "    l = len(s)\n",
    "    for i in range(len(s),n):\n",
    "        s.append(float('nan'))             # could use '' instead of float('nan')\n",
    "    if len(s) > n:\n",
    "        s[n-1] = '+'.join(s[n-1:])\n",
    "    return s[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = str_split_in_n('WCP+FIS+UBCO', 3, descend = True)\n",
    "print(s1)\n",
    "s2 = str_split_in_n('SPC+SPNC+UA+O 65', 6, descend = True)\n",
    "print(s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to select the element in the position <em>series_index</em> in a series created by splitting string <em>s</em> \n",
    "into a series of length <em>series_length</em> using <em>str_split_in_n</em>, using a separator (default value '+'), in ascending order (default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_from_str_split(s, series_length, series_index, sep = '+', descend = False):\n",
    "    return(str_split_in_n(s, series_length, sep, descend)[series_index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the status codes in 4 in descending order and select the 1st to 4th status codes and put them in new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_status_columns = 4     # Number of status columns\n",
    "loc_status_codes = 2       # Column index of combined status codes\n",
    "df3_part1['status_1'] = df3_part1.apply(lambda row: \n",
    "                                        column_status(row[loc_status_codes], num_status_columns, 0, descend = True), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_part1['status_2'] = df3_part1.apply(lambda row: \n",
    "                                        column_status(row[loc_status_codes], num_status_columns, 1, descend = True), axis=1)\n",
    "df3_part1['status_3'] = df3_part1.apply(lambda row: \n",
    "                                        column_status(row[loc_status_codes], num_status_columns, 2, descend = True), axis=1)\n",
    "df3_part1['status_4'] = df3_part1.apply(lambda row: \n",
    "                                        column_status(row[loc_status_codes], num_status_columns, 3, descend = True), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_part1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_part1['status_1'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create flags for each status code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_list = ['CA', 'C-UA', 'C-UB', 'FASS', 'FISH', 'UA', 'UB', 'INTN', 'SEMP', 'SST', 'STEA', 'DA', 'IP', 'OFP', \n",
    "               'OFPJST', 'PRTA', 'SPC', 'WCP', 'O 65', 'BTE', 'BTW', 'FIS', 'PTJI', 'UBCO', 'SPNC', 'SPT', 'LMAF', \n",
    "               'WPGO', 'SPFT', 'YDI', 'SMLH'] # add more?\n",
    "\n",
    "def match_in_range(s, row, start_range, end_range):\n",
    "    match = 0\n",
    "    for i in range(start_range, end_range):\n",
    "        if s == row[i]:\n",
    "            match = 1\n",
    "    return match        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in status_list:\n",
    "    df3_part1[e] = df3_part1.apply(lambda row: match_in_range(e, row, 3, 6), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See all the status codes in column 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_part1['status_1'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See all the status codes in column 2. \n",
    "A code will appear in this column, for each individual, for each time point where they have more than one status. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_part1['status_2'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See all the status codes in column 3.<br>\n",
    "A code will appear in this column, for each individual, for each time point where they have more than one status. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_part1['status_3'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See all the status codes in column 4.<br>\n",
    "A code will appear in this column, for each individual, for each time point where they have more than one status. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_part1['status_4'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find all status values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out all the different status codes in the DataFrame. This may be useful when everything is put back into a single DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_status = np.unique(df3_part1.iloc[:, 3:7].values)\n",
    "all_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.unique(df2.iloc[:, 2:3].values) # This code takes a little while to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export DataFrame to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df3_part1.to_csv('\\\\\\\\cskma0294\\\\F\\\\Evaluations\\\\JobPath\\\\WeeklyStatus_part1_'+today+'.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df3['LM_WSW'] = df3.apply(lambda row: max(row['CA'] + row['C-UA'] + row['C-UB'] + row['FASS'] + row['FISH'] + \n",
    "#                                          row['UA'] + row['UB'] + row['INTN'] + row['SEMP'] + row['SST'] + \n",
    "#                                          row['STEA'], 1), axis=1)\n",
    "#df3['WSW_Non_LM'] = df3.apply(lambda row: max(row['DA'] + row['IP'] + row['OFP'] + row['OFPJST'] + row['PRTA'] + \n",
    "#                                              row['SPC'] + row['WCP'] + row['O 65'], 1), axis=1)\n",
    "#df3['Ed_or_Training'] = df3.apply(lambda row: row['BTE'], axis=1)\n",
    "#df3['Empl'] = df3.apply(lambda row: max(row['BTW'] + row['C-UA'] + row['C-UB'] + row['FIS'] + row['INTN'] + \n",
    "#                                        row['PTJI'] + row['SEMP'] + row['SST'] + row['STEA'], 1), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat for parts 2-10..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the parts back into a single DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3 = pd.concat([df3_part1, df3_part2, df3_part3, df3_part4, df3_part5, df3_part6, df3_part7, df3_part8, df3_part9, df3_part10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df3.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df3.to_csv('\\\\\\\\cskma0294\\\\F\\\\Evaluations\\\\JobPath\\\\WeeklyStatus_flags'+today+'.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <em>Alternative approach to creating flags across different status codes</em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isLR(x):\n",
    "    \n",
    "    lr_types = ['C-UA', 'C-UB', 'UBCO', 'JA', 'JB']\n",
    "    if type(x) is str:\n",
    "        # just returns it untouched\n",
    "        if x in lr_types:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfLR = df2.applymap(isLR)\n",
    "#dfLR.columns = new_columns\n",
    "#dfLR.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
